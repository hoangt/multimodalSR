In order to create and train a network on a dataset, you need to:  

1. Generate a binary file from the source dataset. Your data is expected to be structured as the TIMIT dataset:  
    - WAV files, 16kHz sampling rate, folder structure `dataset/TRAIN/speakerName/videoName/`. 
        Each videoName/ directory contains a `videoName.wav` and `videoName.phn`. 
        The phn contains the audio sample (@16kHz) numbers where each phoneme starts and ends.   
    - If your files are in a different format, you can use functions from fixDataset/ to: (use transform.py, with the appropriate arguments, see bottom of file)   
        - fix wav headers, resample wavs. Store them under `dataRoot/dataset/fixed(nbPhonemes)/`  
        `transform.py phonemes -i dataRoot/TIMIT/original/ -o dataRoot/TIMIT/fixed`
        - fix labelfiles: replace phonemes (eg to use a reduced phoneme set; I used the 39 phonemes from Lee and Hon (1989)).  Stored next to fixed wavs, under `root/dataset/fixed(nbPhonemes)/`  
        - create a MLF file (like from HTK tool, and as used in the TCDTIMIT dataset)   
        - the scripts should be case-agnostic, but you can convert lower to uppercase and vice versa by running `find . -depth -print0 | xargs -0 rename '$_ = lc $_'` in the root dataset directory (change 'lc' to 'uc to convert to upper case). Repeat until you get no more output.  
    - Then set variables in datasetToPkl.py (source and target dir, nbMFCCs to use etc), and run the file   
        - the result is stored as `root/dataset/binary(nbPhonemes)/dataset/dataset_nbPhonemes_ch.pkl`. eg root/TIMIT/binary39/TIMIT/TIMIT_39_ch.pkl  
        - the mean and std_dev of the train data are stored as `root/dataset/binary_nbPhonemes/dataset_MeanStd.pkl`. It's useful for normalization when evaluating. 
1. Use RNN.py to start training. Its functions are implemented in RNN_tools_lstm.py, but you can set the parameters from RNN.py.    
    - set location of pkl generated by datasetToPkl.py  
    - specify number of LSTM layers and number of units per layer  
    - use bidirectional LSTM layers   
    - add some dense layers (though it did not improve performance for me)  
    - learning rate and decay (LR is updated at end of RNN_tools_lstm.py). It's decreased if the performance hasn't improved for some time.    
    
    - it will automatically give the model a name based on the specified parameters. A log file, the model parameters and a pkl file containing training info (accuracy, error etc for each epoch) are stored as well. 
      The storage location is`root/dataset/results`  
        
1. You can use evaluateManyDatasets.py to test your trained network on any (labeled) dataset. (I used TCDTIMIT and TIMIT).  
    - Set the correct network parameters and root dir; it will build the model name and check if the model has been trained.  
    - To increase evaluation speed, preprocessed evaluation data is stored in pkl format, so it doesn't need to be preprocessed each evaluation.  
  
The TIMIT dataset is non-free and available from [https://catalog.ldc.upenn.edu/LDC93S19](https://catalog.ldc.upenn.edu/LDC93S1).    
The TCDTIMIT dataset is publicly available from [https://sigmedia.tcd.ie/TCDTIMIT/](https://sigmedia.tcd.ie/TCDTIMIT/).  
I recommend to use my repo [TCDTIMITprocessing](https://github.com/matthijsvk/TCDTIMITprocessing) to download, exctract the database. You can use `extractTCDTIMITaudio.py` to get the phoneme and wav files.
It also contains scripts for lipreading: extract faces, mouths etc from the video depending on label files.  